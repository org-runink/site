<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Hallucinations on Runink</title>
    <link>https://runink.org/tags/ai-hallucinations/</link>
    <description>Recent content in AI Hallucinations on Runink</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Jul 2025 10:38:54 -0400</lastBuildDate>
    <atom:link href="https://runink.org/tags/ai-hallucinations/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Why Gen AI Hallucinations Can Be a Nightmare for Logistics—And How to Fix Them</title>
      <link>https://runink.org/blog/gen-ai-hallucinations-logistics-solutions/</link>
      <pubDate>Tue, 22 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://runink.org/blog/gen-ai-hallucinations-logistics-solutions/</guid>
      <description>&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;br&gt;&#xA;Hallucinations in generative AI—fabricated, inaccurate information—can wreak havoc in logistics, retail, transportation, and healthcare operations. Techniques like query expansion, reranking, embedding adapters, and model tuning can significantly reduce these errors, improving forecasting, routing, and inventory decisions.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;br&gt;&#xA;&lt;hr&gt;&#xA;&lt;h1 id=&#34;generative-ai-hallucinations-a-logistics-nightmare-and-how-to-stop-them&#34;&gt;Generative AI Hallucinations: A Logistics Nightmare and How to Stop Them&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction-what-are-ai-hallucinations&#34;&gt;Introduction: What Are AI &amp;ldquo;Hallucinations&amp;rdquo;?&lt;/h2&gt;&#xA;&lt;p&gt;Imagine asking an AI for advice on optimizing your delivery routes or forecasting next month’s product demand, and it responds with confident-sounding &lt;strong&gt;nonsense&lt;/strong&gt;. In the world of generative AI, this phenomenon is known as a &lt;em&gt;hallucination&lt;/em&gt;. When a language model hallucinates, &lt;strong&gt;it generates information that seems accurate but is actually false&lt;/strong&gt;. In other words, the AI is essentially making things up – not out of malice, but because it doesn’t know any better given its training and input.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
